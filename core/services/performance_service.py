"""
–°–µ—Ä–≤–∏—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
–í–∫–ª—é—á–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤, –ø—É–ª—ã —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""
import asyncio
import time
import logging
from typing import Dict, Any, Optional, List, Callable
from functools import wraps
from datetime import datetime, timedelta
import json

from .cache import cache_service

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self):
        self.metrics = {}
        self.slow_queries = []
        self.threshold_ms = 1000  # –ü–æ—Ä–æ–≥ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º—Å
    
    def record_query(self, query_name: str, duration_ms: float, params: Dict = None):
        """–ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É –∑–∞–ø—Ä–æ—Å–∞"""
        if query_name not in self.metrics:
            self.metrics[query_name] = {
                'count': 0,
                'total_time': 0,
                'avg_time': 0,
                'min_time': float('inf'),
                'max_time': 0,
                'last_executed': None
            }
        
        metric = self.metrics[query_name]
        metric['count'] += 1
        metric['total_time'] += duration_ms
        metric['avg_time'] = metric['total_time'] / metric['count']
        metric['min_time'] = min(metric['min_time'], duration_ms)
        metric['max_time'] = max(metric['max_time'], duration_ms)
        metric['last_executed'] = datetime.now()
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if duration_ms > self.threshold_ms:
            self.slow_queries.append({
                'query': query_name,
                'duration_ms': duration_ms,
                'params': params,
                'timestamp': datetime.now()
            })
            logger.warning(f"üêå –ú–µ–¥–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {query_name} - {duration_ms:.2f}ms")
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return {
            'metrics': self.metrics,
            'slow_queries_count': len(self.slow_queries),
            'recent_slow_queries': self.slow_queries[-10:],  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            'total_queries': sum(m['count'] for m in self.metrics.values())
        }


class QueryOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self):
        self.monitor = PerformanceMonitor()
        self.cache_ttl = {
            'user_profile': 300,      # 5 –º–∏–Ω—É—Ç
            'partner_info': 600,       # 10 –º–∏–Ω—É—Ç
            'categories': 1800,        # 30 –º–∏–Ω—É—Ç
            'loyalty_config': 900,     # 15 –º–∏–Ω—É—Ç
            'tariffs': 3600,          # 1 —á–∞—Å
            'translations': 7200,     # 2 —á–∞—Å–∞
            'catalog': 30,            # 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è –∫–∞—Ç–∞–ª–æ–≥–∞
        }
    
    def cached_query(self, cache_key: str, ttl: int = 300):
        """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                if cache_key == "catalog":
                    # –î–ª—è –∫–∞—Ç–∞–ª–æ–≥–∞ —Å–æ–∑–¥–∞–µ–º –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ args –∏ kwargs
                    import inspect
                    sig = inspect.signature(func)
                    bound_args = sig.bind(*args, **kwargs)
                    bound_args.apply_defaults()
                    
                    slug = bound_args.arguments.get('slug', 'all')
                    sub_slug = bound_args.arguments.get('sub_slug', 'all')
                    page = bound_args.arguments.get('page', 1)
                    city_id = bound_args.arguments.get('city_id', 'none')
                    unique_key = f"catalog:{slug}:{sub_slug}:{page}:{city_id}"
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π TTL –¥–ª—è –∫–∞—Ç–∞–ª–æ–≥–∞
                    ttl = self.cache_ttl.get('catalog', 30)
                else:
                    unique_key = cache_key
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
                cached_result = await cache_service.get(unique_key)
                if cached_result:
                    logger.warning(f"üîß CACHE HIT: {unique_key} - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                    return json.loads(cached_result)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
                logger.warning(f"üîß CACHE MISS: {unique_key} - –≤—ã–ø–æ–ª–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é")
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    duration_ms = (time.time() - start_time) * 1000
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
                    self.monitor.record_query(func.__name__, duration_ms, kwargs)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                    await cache_service.set(unique_key, json.dumps(result), ex=ttl)
                    logger.debug(f"üíæ Cached: {unique_key} (TTL: {ttl}s)")
                    
                    return result
                except Exception as e:
                    duration_ms = (time.time() - start_time) * 1000
                    self.monitor.record_query(func.__name__, duration_ms, {'error': str(e)})
                    raise
            
            return wrapper
        return decorator
    
    def batch_query(self, queries: List[Callable], batch_size: int = 10):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –±–∞—Ç—á–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        async def execute_batch():
            results = []
            for i in range(0, len(queries), batch_size):
                batch = queries[i:i + batch_size]
                batch_results = await asyncio.gather(*batch, return_exceptions=True)
                results.extend(batch_results)
            return results
        return execute_batch


class DatabaseOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.connection_pools = {}
        self.query_cache = {}
    
    async def optimize_connection_pool(self, service_name: str, max_connections: int = 20):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        try:
            if service_name == 'postgresql':
                from core.database.postgresql_service import get_postgresql_service
                service = get_postgresql_service()
                await service.init_pool()
                logger.info(f"üîß Optimized PostgreSQL connection pool: max_connections={max_connections}")
            
            elif service_name == 'redis':
                from core.services.cache import get_cache_service
                cache = await get_cache_service()
                await cache.ping()
                logger.info(f"üîß Optimized Redis connection")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to optimize {service_name} connection pool: {e}")
    
    def create_indexes(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_users_telegram_id ON users(telegram_id)",
            "CREATE INDEX IF NOT EXISTS idx_partners_tg_user_id ON partners_v2(tg_user_id)",
            "CREATE INDEX IF NOT EXISTS idx_cards_category_id ON cards_v2(category_id)",
            "CREATE INDEX IF NOT EXISTS idx_cards_status ON cards_v2(status)",
            "CREATE INDEX IF NOT EXISTS idx_points_history_user_id ON points_history(user_id)",
            "CREATE INDEX IF NOT EXISTS idx_points_history_created_at ON points_history(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_categories_active ON categories_v2(is_active)",
            "CREATE INDEX IF NOT EXISTS idx_tariffs_active ON tariffs(is_active)",
        ]
        
        try:
            from core.database.db_v2 import get_connection
            with get_connection() as conn:
                for index_sql in indexes:
                    conn.execute(index_sql)
                conn.commit()
                logger.info("üîß Database indexes created successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to create database indexes: {e}")
    
    async def analyze_query_performance(self):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            from core.database.db_v2 import get_connection
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–∞–±–ª–∏—Ü
            with get_connection() as conn:
                cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                analysis = {}
                for table in tables:
                    cursor = conn.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    analysis[table] = {'rows': count}
                
                logger.info(f"üìä Database analysis: {analysis}")
                return analysis
                
        except Exception as e:
            logger.error(f"‚ùå Failed to analyze database performance: {e}")
            return {}


class PerformanceService:
    """–ì–ª–∞–≤–Ω—ã–π —Å–µ—Ä–≤–∏—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.optimizer = QueryOptimizer()
        self.db_optimizer = DatabaseOptimizer()
        self.monitor = PerformanceMonitor()
        self.is_initialized = False
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        if self.is_initialized:
            return
        
        try:
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            await self.db_optimizer.optimize_connection_pool('postgresql')
            await self.db_optimizer.optimize_connection_pool('redis')
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
            self.db_optimizer.create_indexes()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            await self.db_optimizer.analyze_query_performance()
            
            self.is_initialized = True
            logger.info("üöÄ Performance service initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize performance service: {e}")
    
    async def get_performance_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        stats = self.monitor.get_stats()
        stats['is_initialized'] = self.is_initialized
        stats['cache_ttl_config'] = self.optimizer.cache_ttl
        return stats
    
    async def optimize_slow_queries(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        slow_queries = self.monitor.slow_queries
        if not slow_queries:
            logger.info("‚úÖ No slow queries found")
            return
        
        logger.warning(f"üêå Found {len(slow_queries)} slow queries, optimizing...")
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        # –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
        
        # –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.monitor.slow_queries.clear()
        logger.info("‚úÖ Slow queries optimized")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
performance_service = PerformanceService()


# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def cached_query(cache_key: str, ttl: int = 300):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
    return performance_service.optimizer.cached_query(cache_key, ttl)


def monitor_performance(func_name: str = None):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                duration_ms = (time.time() - start_time) * 1000
                performance_service.monitor.record_query(
                    func_name or func.__name__, 
                    duration_ms, 
                    kwargs
                )
                return result
            except Exception as e:
                duration_ms = (time.time() - start_time) * 1000
                performance_service.monitor.record_query(
                    func_name or func.__name__, 
                    duration_ms, 
                    {'error': str(e)}
                )
                raise
        return wrapper
    return decorator


__all__ = [
    'PerformanceService',
    'performance_service',
    'cached_query',
    'monitor_performance',
    'QueryOptimizer',
    'DatabaseOptimizer',
    'PerformanceMonitor'
]
